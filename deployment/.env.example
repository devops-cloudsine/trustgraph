# =============================================================================
# TrustGraph Docker Compose Environment Configuration
# =============================================================================
# Copy this file to .env and modify values as needed:
#   cp .env.example .env
#
# Docker Compose automatically loads .env from the same directory as
# docker-compose.yaml.
# =============================================================================

# -----------------------------------------------------------------------------
# VLLM / LLM Configuration
# -----------------------------------------------------------------------------
# The model to run in vLLM
# Examples:
#   - Qwen/Qwen3-VL-4B-Instruct (default, 4B vision-language model)
#   - meta-llama/Llama-3.2-11B-Vision-Instruct
#   - mistralai/Mistral-7B-Instruct-v0.3
#   - Qwen/Qwen2.5-7B-Instruct
VLLM_MODEL=Qwen/Qwen3-VL-4B-Instruct

# Maximum model context length (tokens)
# Lower values reduce memory usage; higher values allow longer contexts
VLLM_MAX_MODEL_LEN=16384

# GPU memory utilization (0.0 - 1.0)
# Higher values use more VRAM but may cause OOM errors
# Recommended: 0.85 for single GPU, 0.80 for multi-GPU
VLLM_GPU_MEMORY_UTILIZATION=0.85

# Data type for model weights
# Options: auto, float16, bfloat16, float32
# auto = let vLLM decide based on GPU capabilities
VLLM_DTYPE=auto

# External port for vLLM API (internal container port is always 8000)
VLLM_PORT=8001

# -----------------------------------------------------------------------------
# Hugging Face Configuration
# -----------------------------------------------------------------------------
# Your Hugging Face Hub token for accessing gated models (e.g., Llama, Mistral)
# Get one at: https://huggingface.co/settings/tokens
# Leave empty if using only public models
HUGGING_FACE_HUB_TOKEN=

# -----------------------------------------------------------------------------
# File Paths
# -----------------------------------------------------------------------------
# Host path for files to be parsed/processed
# This directory is mounted into containers for document processing
FILES_TO_PARSE_HOST_PATH=/home/ubuntu/Github/trustgraph/files_to_parse

# Path to trustgraph repository (for development volume mounts)
# Used by OCR services for live code reloading during development
TRUSTGRAPH_DEV_PATH=/home/ubuntu/Github/trustgraph

# -----------------------------------------------------------------------------
# TrustGraph Version
# -----------------------------------------------------------------------------
# Version tag for trustgraph Docker images
# Update this to upgrade all trustgraph services at once
TRUSTGRAPH_VERSION=1.4.23

# -----------------------------------------------------------------------------
# API Secrets
# -----------------------------------------------------------------------------
# Gateway API secret (for authenticating API requests)
GATEWAY_SECRET=

# MCP Server secret
MCP_SERVER_SECRET=

# OpenAI-compatible API configuration
# Set these if using an external OpenAI-compatible API instead of local vLLM
OPENAI_BASE_URL=
OPENAI_TOKEN=
